{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Training Proess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, start with Inline plotting, imports, and global variables used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global vatiables list:**  \n",
    "NAME: nickname for our model, must match the folder name in DL folder  \n",
    "INP: Directory where we will read our input file  \n",
    "DIR: Directory where we'll save model and export our parameters  \n",
    "cat_vars: List of categorical variables in our model  \n",
    "cont_vars: List of continous variables in our model  \n",
    "QP: Quantization Parameter  \n",
    "Layers: Number of neurons per hidden layer in our network  \n",
    "Dropouts: Percentage of dropout rate per hidden layer  \n",
    "BN_use: Use batch normalization if set to True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Global Variable Declarations\n",
    "from fastai.tabular import *\n",
    "\n",
    "NAME='blowing'\n",
    "INP='./DL'\n",
    "DIR='./DL/{0}'.format(NAME)\n",
    "cat_vars = ['Height', 'Width']\n",
    "cont_vars = ['top_left', 'top_center', 'top_right', 'left', 'center', 'right', \n",
    "             'bottom_left', 'bottom_center', 'bottom_right']\n",
    "QP=22\n",
    "Layers=[22, 20]\n",
    "Dropouts=[0.001, 0.01]\n",
    "BN_use=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to help us keep the code concise  \n",
    "get_cv_idx(): return random list of indices from a list given a percentage value \"borrowed from FastAIv0.7\"  \n",
    "read_proc(): Reads the input file, normalize and categorify, and return FastAI TabularDataBunch and Learner methods  \n",
    "export_mapper(): export means and stds used for normalization \"to be used for inference\"  \n",
    "save_model(): saves our model for later use  \n",
    "export_parameters(): export each layer's weights and biases \"to be used for inference\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def get_cv_idxs(n, cv_idx=0, val_pct=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    n_val = int(val_pct*n)\n",
    "    idx_start = cv_idx*n_val\n",
    "    idxs = np.random.permutation(n)\n",
    "    return idxs[idx_start:idx_start+n_val]\n",
    "\n",
    "def read_proc():\n",
    "    df = pd.read_csv('{0}/SSE_{1}.csv'.format(INP, QP), names=cont_vars+cat_vars+['y'])\n",
    "    procs = [Categorify]\n",
    "    normz = Normalize(cat_vars, cont_vars)\n",
    "    normz(df)\n",
    "    valid_idx = get_cv_idxs(len(df))\n",
    "    data = TabularDataBunch.from_df(INP, df, 'y', valid_idx=valid_idx, procs=procs, cat_names=cat_vars, \n",
    "                                    cont_names=cont_vars)\n",
    "    learn = tabular_learner(data, layers=Layers, metrics=accuracy, emb_drop=0.001, ps=Dropouts, use_bn=BN_use)\n",
    "    return df, normz, valid_idx, data, learn\n",
    "\n",
    "def export_mapper():\n",
    "    mapper_df = pd.DataFrame(index=['mean', 'std'], columns=cont_vars)\n",
    "    for column in cont_vars:\n",
    "        mapper_df[column].loc['mean'] = normz.means[column]\n",
    "        mapper_df[column].loc['std'] = normz.stds[column]\n",
    "    mapper_df.to_csv('{0}/{1}/mapper_{1}.csv'.format(DIR, QP), index=False, header=None, line_terminator=';\\n')    \n",
    "    return\n",
    "    \n",
    "def save_model():\n",
    "    acc = learn.recorder.metrics[-1][0].numpy() * 100\n",
    "    learn.save(f'QP{QP}_{NAME}_acc{(acc.round(2))}')\n",
    "    return\n",
    "\n",
    "def export_parameters():\n",
    "    e = l = bn = 0\n",
    "    bn_list = ['weight', 'bias', 'running_mean', 'running_var']\n",
    "    for idx, layer in enumerate(learn.layer_groups[0]):\n",
    "        if isinstance(layer, nn.Embedding):\n",
    "            pd.DataFrame(learn.layer_groups[0][idx].weight.data.numpy()).to_csv('{0}/{1}/emb{2}-weight.csv'.format(DIR, QP, e), \n",
    "                                                                       index=False, header=None, line_terminator= ',\\n')\n",
    "            e+=1\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            pd.DataFrame(learn.layer_groups[0][idx].weight.data.numpy()).to_csv('{0}/{1}/lins{2}-weight.csv'.format(DIR, QP, l), \n",
    "                                                                     index=False, header=None, line_terminator= ',\\n')\n",
    "            pd.DataFrame(learn.layer_groups[0][idx].bias.data.numpy()).to_csv('{0}/{1}/lins{2}-bias.csv'.format(DIR, QP, l), \n",
    "                                                                   index=False, header=None, line_terminator=\", \")\n",
    "            l+=1\n",
    "        if isinstance(layer, nn.BatchNorm1d):\n",
    "            for i in bn_list:\n",
    "                pd.DataFrame(getattr(learn.layer_groups[0][idx], i).data.numpy()).to_csv('{0}/{1}/bns{2}-{3}.csv'.format(DIR, QP, bn, i), \n",
    "                                                                         index=False, header=None, line_terminator=', ')\n",
    "            bn += 1\n",
    "    for i in bn_list:\n",
    "        pd.DataFrame(getattr(learn.model.bn_cont, i).data.numpy()).to_csv('{0}/{1}/bn-{2}.csv'.format(DIR, QP, i), \n",
    "                                                                 index=False, header=None, line_terminator=', ')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in globals(): del df, valid_idx, data, learn\n",
    "df, normz, valid_idx, data, learn = read_proc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_mapper()\n",
    "save_model()\n",
    "export_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for QP in [27, 32, 37]:\n",
    "    if 'df' in globals(): del df, valid_idx, data, learn\n",
    "    df, valid_idx, data, learn = read_proc()\n",
    "    learn.fit_one_cycle(1, 1e-2)\n",
    "    export_mapper()\n",
    "    save_model()\n",
    "    export_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTATION IN C++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement our model in HM-Tool, we need to know the values of weights and biases\n",
    "To get a summary of all the layers used in our model, run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will vary depending on the model, but in general, you can see the following modules:  \n",
    "**Linear:** used to multiply by weight and add bias for each layer (see torch.nn.linear)  \n",
    "**BatchNorm1d:** used for batch normalization of each layer(see torch.nn.batchnorm1d)  \n",
    "**outp:** Linear layer that is used for output  \n",
    "There are also Embeddings, which are not used in this project  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reach any parameters we need from with m.model.\"module\"  \n",
    "Example: Find weights of first linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.model.layers[0].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters that are of importance to us are:  \n",
    "**weight:** The weights of each layer (found in lins, bns, bn, outp)  \n",
    "**bias:** Biases of each layer (found in lins, bns, bn, outp)  \n",
    "**running_mean:** The running mean computed during training - used for BatchNorm (found in bns, bn)  \n",
    "**running_var:** The running variance computed during training - used for BatchNorm (found in bns, bn)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show all required parameters \"including running_mean and running_var\" run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
