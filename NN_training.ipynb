{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Training Proess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports & Global variables list:**  \n",
    "A symbolic link is done to import FastAI v0.7 library, the target folder is named \"fastai07\"  \n",
    "\n",
    "NAME: nickname for our model, must match the folder name in DL folder  \n",
    "INP: Directory where we will read our input file  \n",
    "DIR: Directory where we'll save model and export our parameters  \n",
    "cat_vars: List of categorical variables in our model  \n",
    "cont_vars: List of continous variables in our model  \n",
    "QP: Quantization Parameter  \n",
    "Layers: Number of neurons per hidden layer in our network  \n",
    "Dropouts: Percentage of dropout rate per hidden layer  \n",
    "BN_use: Use batch normalization if set to True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai07.structured import *\n",
    "from fastai07.column_data import *\n",
    "\n",
    "NAME='binary'\n",
    "INP='./DL'\n",
    "DIR='./DL/{0}'.format(NAME)\n",
    "cat_vars = ['Height', 'Width']\n",
    "cont_vars = ['top_left', 'top_center', 'top_right', 'left', 'center', 'right', \n",
    "             'bottom_left', 'bottom_center', 'bottom_right']\n",
    "maplist = [['top_left', 0], ['top_center', 1], ['top_right', 2], ['left', 3], ['center', 4], ['right', 5], \n",
    "         ['bottom_left', 6], ['bottom_center', 7], ['bottom_right', 8]]\n",
    "QP=22\n",
    "Layers=[40, 40, 40]\n",
    "Dropouts=[0.001, 0.01]\n",
    "BN_use=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions**:  \n",
    "In FastAI v0.7, there's a proc_df() function that does the following:  \n",
    "1) Splits dependent variable \"output\" from the dataframe  \n",
    "2) Normalizes the continuous variables  \n",
    "3) Returns a mapper that holds the mean and std for normalization  \n",
    "4) Categorizes the categorical variables  \n",
    "5) Handles missing values  \n",
    "\n",
    "This function will be used in read_proc() process our data. Note that we don't have any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read, normalize, and process Inputs and Outputs\n",
    "def read_proc():\n",
    "    df = pd.read_csv('{0}/SSE_{1}.csv'.format(INP, QP), names=cont_vars+cat_vars+['y'])\n",
    "    for v in cat_vars: df[v] = df[v].astype('category').cat.as_ordered()\n",
    "    for v in cont_vars: df[v] = df[v].astype('float32')\n",
    "    df, y, nas, mapper = proc_df(df, 'y',do_scale=True)\n",
    "    del nas\n",
    "    for v in cat_vars: df[v] = df[v].astype('category').cat.as_ordered()\n",
    "    for v in cont_vars: df[v] = df[v].astype('float32')\n",
    "    cat_sz = [(c, len(df[c].cat.categories)+1) for c in cat_vars]\n",
    "    # Rule of Thumb for embedding sizes \"taken from FastAI course 2018\"\n",
    "    emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]\n",
    "    val_idx = get_cv_idxs(len(df), val_pct=0.2)\n",
    "    return df, y, mapper, emb_szs, val_idx\n",
    "\n",
    "# Export the Mean and STDev\n",
    "# The target folder is: ./$DIR/$QP/\n",
    "def export_mapper():\n",
    "    mapper_df = pd.DataFrame(index=['mean', 'std'], columns=cont_vars)\n",
    "    for column,i in maplist:\n",
    "        mapper_df[column].loc['mean'] = np.float64(mapper.features[i][1].mean_)\n",
    "        mapper_df[column].loc['std'] = np.float64(np.sqrt(mapper.features[i][1].var_))\n",
    "\n",
    "    mapper_df.to_csv('{0}/{1}/mapper_{1}.csv'.format(DIR, QP), index=False, header=None, line_terminator=\";\\n\")\n",
    "    return\n",
    "\n",
    "# Process and Save model\n",
    "# The saved model is in the format: QP{qp}_{name}_{accuracy}\n",
    "def save_model():\n",
    "    log_preds, targs = m.predict_with_targs()\n",
    "    preds = np.argmax(log_preds, axis=1)\n",
    "    right = 0\n",
    "    for i in range (len(preds)):\n",
    "        if(preds[i] == targs[i]):\n",
    "            right += 1\n",
    "    acc = (right / len(preds)) * 100\n",
    "    print(\"Validation accuracy: \", acc)\n",
    "    name = \"QP{0}_{1}_acc{2}\".format(QP, NAME, round(acc, 2))\n",
    "    m.save(name)\n",
    "    return\n",
    "\n",
    "# Export Weights and Biases for each layer\n",
    "# The target folder is: ./$DIR/$QP/\n",
    "def export_parameters():\n",
    "    for i in range(len(m.model.bns)):\n",
    "        pd.DataFrame(m.model.bns[i].weight.data.numpy()).to_csv('{0}/{1}/bns{2}-weight.csv'.format(DIR, QP, i), \n",
    "                                                                index=False, header=None , line_terminator=\", \")\n",
    "        pd.DataFrame(m.model.bns[i].bias.data.numpy()).to_csv('{0}/{1}/bns{2}-bias.csv'.format(DIR, QP, i), \n",
    "                                                              index=False, header=None , line_terminator=\", \")\n",
    "    for i in range(len(m.model.lins)):\n",
    "        pd.DataFrame(m.model.lins[i].weight.sign().data.numpy()).to_csv('{0}/{1}/lins{2}-weight.csv'.format(DIR, QP, i), \n",
    "                                                                 index=False, header=None, line_terminator=\",\\n\")\n",
    "        pd.DataFrame(m.model.lins[i].bias.sign().data.numpy()).to_csv('{0}/{1}/lins{2}-bias.csv'.format(DIR, QP, i), \n",
    "                                                               index=False, header=None, line_terminator=\", \")\n",
    "    for i in range(len(m.model.embs)):\n",
    "        pd.DataFrame(m.model.embs[i].weight.data.numpy()).to_csv('{0}/{1}/emb{2}-weight.csv'.format(DIR, QP, i), \n",
    "                                                                index=False, header=None, line_terminator=\",\\n\")\n",
    "    pd.DataFrame(m.model.outp.weight.sign().data.numpy()).to_csv('{0}/{1}/outp-weight.csv'.format(DIR, QP), \n",
    "                                                          index=False, header=None, line_terminator=\",\\n\")\n",
    "    pd.DataFrame(m.model.outp.bias.sign().data.numpy()).to_csv('{0}/{1}/outp-bias.csv'.format(DIR, QP), index=False, \n",
    "                                                        header=None, line_terminator=\", \")\n",
    "    pd.DataFrame(m.model.bn.weight.data.numpy()).to_csv('{0}/{1}/bn-weight.csv'.format(DIR, QP), index=False, \n",
    "                                                        header=None, line_terminator=\", \")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Binarize(tensor,quant_mode='det'):\n",
    "    if quant_mode=='det':\n",
    "        return tensor.sign()\n",
    "    else:\n",
    "        return tensor.add_(1).div_(2).add_(torch.rand(tensor.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)\n",
    "\n",
    "class BinaryLinear(nn.Linear):\n",
    "\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BinaryLinear, self).__init__(*kargs, **kwargs)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        if input.size(1) != 17:\n",
    "            input.data=Binarize(input.data)\n",
    "        if not hasattr(self.weight,'org'):\n",
    "            self.weight.org=self.weight.data.clone()\n",
    "        self.weight.data=Binarize(self.weight.org)\n",
    "        out = nn.functional.linear(input, self.weight)\n",
    "        if not self.bias is None:\n",
    "            self.bias.org=self.bias.data.clone()\n",
    "            out += self.bias.view(1, -1).expand_as(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryModel(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops,\n",
    "                 y_range=None, use_bn=False, is_reg=True, is_multi=False):\n",
    "        super().__init__()\n",
    "        for i,(c,s) in enumerate(emb_szs): assert c > 1, f\"cardinality must be >=2, got emb_szs[{i}]: ({c},{s})\"\n",
    "        if is_reg==False and is_multi==False: assert out_sz >= 2, \"For classification with out_sz=1, use is_multi=True\"\n",
    "        self.embs = nn.ModuleList([nn.Embedding(c, s) for c,s in emb_szs])\n",
    "        for emb in self.embs: emb_init(emb)\n",
    "        n_emb = sum(e.embedding_dim for e in self.embs)\n",
    "        self.n_emb, self.n_cont=n_emb, n_cont\n",
    "        \n",
    "        szs = [n_emb+n_cont] + szs\n",
    "        self.lins = nn.ModuleList([\n",
    "            BinaryLinear(szs[i], szs[i+1]) for i in range(len(szs)-1)])    # UPDATED\n",
    "        self.bns = nn.ModuleList([\n",
    "            nn.BatchNorm1d(sz) for sz in szs[1:]])\n",
    "        for o in self.lins: kaiming_normal(o.weight.data)\n",
    "        self.outp = BinaryLinear(szs[-1], out_sz)    # UPDATED\n",
    "        kaiming_normal(self.outp.weight.data)\n",
    "\n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        self.drops = nn.ModuleList([nn.Dropout(drop) for drop in drops])\n",
    "        self.bn = nn.BatchNorm1d(n_cont)\n",
    "        self.use_bn,self.y_range = use_bn,y_range\n",
    "        self.is_reg = is_reg\n",
    "        self.is_multi = is_multi\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embs)]\n",
    "            x = torch.cat(x, 1)\n",
    "#             x = Binarize(x)  # UPDATED\n",
    "#             x = self.emb_drop(x)\n",
    "        if self.n_cont != 0:\n",
    "            x2 = self.bn(x_cont)\n",
    "#             x2 = Binarize(x2)  # UPDATED\n",
    "            x = torch.cat([x, x2], 1) if self.n_emb != 0 else x2\n",
    "\n",
    "        # UPDATED # UPDATED\n",
    "        for l,d, b in zip(self.lins, self.drops, self.bns):\n",
    "            x = l(x)\n",
    "            if self.use_bn: x = b(x)\n",
    "            x = F.hardtanh(x, inplace=True)\n",
    "#             x = d(x)\n",
    "        # END OF UPDATE\n",
    "        \n",
    "        x = self.outp(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnarBinaryData(ModelData):\n",
    "    def __init__(self, path, trn_ds, val_ds, bs, test_ds=None, shuffle=True):\n",
    "        test_dl = DataLoader(test_ds, bs, shuffle=False, num_workers=1) if test_ds is not None else None\n",
    "        super().__init__(path, DataLoader(trn_ds, bs, shuffle=shuffle, num_workers=1),\n",
    "            DataLoader(val_ds, bs*2, shuffle=False, num_workers=1), test_dl)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_data_frames(cls, path, trn_df, val_df, trn_y, val_y, cat_flds, bs=64, is_reg=True, is_multi=False, test_df=None, shuffle=True):\n",
    "        trn_ds  = ColumnarDataset.from_data_frame(trn_df,  cat_flds, trn_y, is_reg, is_multi)\n",
    "        val_ds  = ColumnarDataset.from_data_frame(val_df,  cat_flds, val_y, is_reg, is_multi)\n",
    "        test_ds = ColumnarDataset.from_data_frame(test_df, cat_flds, None,  is_reg, is_multi) if test_df is not None else None\n",
    "        return cls(path, trn_ds, val_ds, bs, test_ds=test_ds, shuffle=shuffle)\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frame(cls, path, val_idxs, df, y, cat_flds, bs=64, is_reg=True, is_multi=False, test_df=None, shuffle=True):\n",
    "        ((val_df, trn_df), (val_y, trn_y)) = split_by_idx(val_idxs, df, y)\n",
    "        return cls.from_data_frames(path, trn_df, val_df, trn_y, val_y, cat_flds, bs, is_reg, is_multi, test_df=test_df, shuffle=shuffle)\n",
    "\n",
    "    def get_learner(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops,\n",
    "                    y_range=None, use_bn=False, **kwargs):\n",
    "        model = BinaryModel(emb_szs, n_cont, emb_drop, out_sz, szs, drops, y_range, use_bn, self.is_reg, self.is_multi)\n",
    "        return StructuredLearner(self, StructuredModel(to_gpu(model)), opt_fn=optim.Adam, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in globals(): del df, y, mapper\n",
    "df, y, mapper, emb_szs, val_idx = read_proc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarBinaryData.from_data_frame(INP, val_idx, df, y, cat_flds=cat_vars, \n",
    "                                       bs=1024, is_multi=False, is_reg=False)\n",
    "m = md.get_learner(emb_szs, len(df.columns) - len(cat_vars),\n",
    "                   0.001, 49, Layers, Dropouts, use_bn=BN_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.lr_find()\n",
    "m.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "m.fit(LR, 5, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('5-Binary2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "m.fit(LR, 5, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('10-Binary2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "m.fit(LR, 5, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('15-Binary2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "m.fit(LR, 5, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('20-Binary2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "m.fit(LR, 50, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Binarize(m.model.lins[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.model.lins[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_data_frame(INP, val_idx, df, y, cat_flds=cat_vars, \n",
    "                                       bs=1024, is_multi=False, is_reg=False)\n",
    "m = md.get_learner(emb_szs, len(df.columns) - len(cat_vars),\n",
    "                   0.001, 49, [22, 20], Dropouts, use_bn=BN_use)\n",
    "m.load(\"QP22_blowing_200_train_acc36.51\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(1e-3, 1, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds, targs = m.predict_with_targs()\n",
    "preds = np.argmax(log_preds, axis=1)\n",
    "mt = np.zeros(49, dtype=np.uint)\n",
    "for targ in targs:\n",
    "    mt[targ] += 1\n",
    "mp = np.zeros(49, dtype=np.uint)\n",
    "for pred in preds:\n",
    "    mp[pred] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num=1, figsize=(15, 10), dpi=100)\n",
    "plt.bar(np.arange(0, 49, 1), mp, width=0.4, color='k', label=\"Predictions\", align='edge')\n",
    "plt.bar(np.arange(0, 49, 1)+0.45, mt, width=0.4, color='b', label=\"Targets\", align='edge')\n",
    "plt.title(\"Fractional Point Statistics\")\n",
    "plt.xlabel(\"Fractional Location\")\n",
    "plt.ylabel(\"Number of predictions\")\n",
    "plt.xticks(np.arange(0, 49, 1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num=1, figsize=(15, 10), dpi=100)\n",
    "plt.bar(np.arange(0, 49, 1), mp, width=0.4, color='k', label=\"Predictions\", align='edge')\n",
    "plt.bar(np.arange(0, 49, 1)+0.45, mt, width=0.4, color='b', label=\"Targets\", align='edge')\n",
    "plt.title(\"Fractional Point Statistics\")\n",
    "plt.xlabel(\"Fractional Location\")\n",
    "plt.ylabel(\"Number of predictions\")\n",
    "plt.xticks(np.arange(0, 49, 1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds, targs = m.predict_with_targs()\n",
    "preds = np.argmax(log_preds, axis=1)\n",
    "mt = np.zeros(49, dtype=np.uint)\n",
    "for targ in targs:\n",
    "    mt[targ] += 1\n",
    "mp = np.zeros(49, dtype=np.uint)\n",
    "for pred in preds:\n",
    "    mp[pred] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num=1, figsize=(15, 10), dpi=100)\n",
    "plt.bar(np.arange(0, 49, 1), mp, width=0.4, color='k', label=\"Predictions\", align='edge')\n",
    "plt.bar(np.arange(0, 49, 1)+0.45, mt, width=0.4, color='b', label=\"Targets\", align='edge')\n",
    "plt.title(\"Fractional Point Statistics\")\n",
    "plt.xlabel(\"Fractional Location\")\n",
    "plt.ylabel(\"Number of predictions\")\n",
    "plt.xticks(np.arange(0, 49, 1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num=1, figsize=(15, 10), dpi=100)\n",
    "plt.bar(np.arange(0, 49, 1), mp, width=0.4, color='k', label=\"Predictions\", align='edge')\n",
    "plt.bar(np.arange(0, 49, 1)+0.45, mt, width=0.4, color='b', label=\"Targets\", align='edge')\n",
    "plt.title(\"Fractional Point Statistics\")\n",
    "plt.xlabel(\"Fractional Location\")\n",
    "plt.ylabel(\"Number of predictions\")\n",
    "plt.xticks(np.arange(0, 49, 1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.load('15-Binary2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.model.lins[0].bias.sign().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_mapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "seaborn.heatmap(sklearn.metrics.confusion_matrix(targs, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read and process our input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in globals(): del df, y, mapper\n",
    "df, y, mapper, emb_szs, val_idx = read_proc()\n",
    "# Optional: export mapper for inference\n",
    "# export_mapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ModelData and Learner objects  \n",
    "The problem we're dealing with is a multi-class classification \"output is one of 49 fractional locations\", so in get_learner() function, we set is_multi= and is_reg= to False  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_data_frame(INP, val_idx, df, y, cat_flds=cat_vars, \n",
    "                                       bs=1024, is_multi=False, is_reg=False)\n",
    "m = md.get_learner(emb_szs, len(df.columns) - len(cat_vars),\n",
    "                   0.001, 49, Layers, Dropouts, use_bn=BN_use)\n",
    "# Optional: Load previously saved model\n",
    "# m.load('QP22_blowing_200_train_acc36.51')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run learning Rate Finder:  \n",
    "Plots the error for each learning rate, we choose the learning rate before the error starts flattening out \"in our case, usually learning rate = 1e-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.lr_find()\n",
    "m.sched.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model for a specified number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LR = 1e-2\n",
    "m.fit(LR, 1, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model and Export parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model()\n",
    "export_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for other Quantization Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for QP in [27, 32, 37]:\n",
    "    if 'df' in globals(): del df, y, mapper\n",
    "    df, y, mapper, emb_szs, val_idx = read_proc()\n",
    "    export_mapper()\n",
    "    md = ColumnarModelData.from_data_frame(INP, val_idx, df, y, cat_flds=cat_vars, \n",
    "                                           bs=1024, is_multi=False, is_reg=False)\n",
    "    m = md.get_learner(emb_szs, len(df.columns) - len(cat_vars),\n",
    "                       0.001, 49, Layers, Dropouts, use_bn=BN_use)\n",
    "    m.fit(LR, 200, metrics=[accuracy])\n",
    "    save_model()\n",
    "    export_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
